---
title: "Exploratory Data Analysis"
output: html_document
---

```{r global_options, include=FALSE}
source(here::here("code", "01_packages.R"))
source(here::here("code/02_knitr-global.R"))
source(here::here("code/03_load_df.R"))
```

## Initial observations

The current status of the working file with the fesshole data is found on the set `fesshole_new`

```{r}
fesshole_new %>% 
  glimpse()
```

The number of rows my differ upon updates done to the file.

One of the first observations we can make is the number of words in each confession.  That way we
can identify the longest confession, and when it was submitted.

### The longest confession?

Which one is the longest confession to date.  We will use the `fesshole_wordcount` data set.


Now let's find out the entry with the maximumn number of words

```{r}
fesshole_wordcount %>% 
  slice(which.max(wordcount)) %>% 
  pull()
```

And when this entry was submitted...


```{r}
fesshole_wordcount %>% 
  slice(which.max(wordcount)) %>% 
  select(timestamp) %>% 
  pull()

```

It was submitted on `r fesshole_new %>% mutate(wordcount_est = stringr::str_count(confessions, '[\\w\']+')) %>% slice(which.max(wordcount_est)) %>% select(timestamp) %>% pull()`

Here's the text of the longest confession: a very juicy account of identity theft, gay politicians, lots of alcohol and the Brexit.

```{r}
fesshole_wordcount %>% 
  slice(which.max(wordcount)) %>% 
  select(confessions) %>% 
  pull()

```

Oh wow. This is a very juicy account of identity theft, gay politicians, lots of alcohol and the Brexit.

The paragraph contains line breaks `\n` that somehow made it into the calculation of the number of words.
That is why the number of words is marked as estimate.

These are the top 10 longest confessions.

```{r}
fesshole_wordcount %>% 
  arrange(desc(wordcount)) %>% 
  head(10)
```

## How many confessions?

There is a lot of counting going on when we do EDA.

For instance, we can ask how many confessions were submitted in a given day, through the months, and also in 
years 2019 and 2020 so far.

We could start by counting the number of confessions submitted every month ever since the beginning of the 
account in July 2019.

### ... per month

Let's get the table count for each month

```{r}
fesshole_detailed %>% 
  count(confess_id, month, year) %>% 
  group_by(year, month) %>% 
  summarise(sum(n))
  
  
```

Now let's see it as a bar chart

```{r}
fesshole_detailed %>% 
  count(timestamp, month, year) %>% 
  group_by(year, month) %>% 
  summarise(confess_count = sum(n)) %>% 
  ungroup() %>% 
  ggplot(aes(month, confess_count, fill = month)) + 
    geom_col() +
    facet_wrap(~ year) +
    theme(axis.text.x = element_text(angle = 90)) 

```

## Which are the hours of the day that are most active?

Let's find ou when people generally send the most confessions.  We can guess it  is the nighttime.


```{r}
fesshole_detailed %>% 
  mutate(time = as_factor(lubridate::hour(timestamp))) %>% 
  count(time) %>% 
  rename(confess_count = n) %>% 
  ggplot(aes(time, confess_count)) + 
    geom_col(fill = "deepskyblue2") +
    theme_classic()


```

As we can see, indeed the most confession are during 7pm, when people are out of work, or school.

Being the lowest number of submissions sent at 4am.

This is taking into account my system time settings.  Most of these confession may be from the UK.
I ignore if these confessions are also coming from other English speaking countries where time difference
is very considerate, since this may actually have an impact on the entries.

## What is the average number of words per entry?

We saw the largest confession has 230 words (more or less),  now let's see what is the average 
number using a histogram and a boxplot.

```{r}
fesshole_wordcount %>% 
  ggplot(aes(wordcount)) + 
    geom_histogram(bins = 45)

```

The vast majority of the confessions has about 35-40 words.  After the 60 count mark we start getting 
fewer submissions with longer amount of words.

Let's filter those confessions with 60 or more words.

```{r}
fesshole_wordcount%>% 
  filter(wordcount >= 60) %>% 
  ggplot(aes(wordcount)) + 
    geom_histogram(bins = 30)
```

It is considerable the reduction in the number of entries of 60 words or less.  A few more than 300 has 60 words or less.  The numbers having 100 up to the max of 230 is very very rare (even less then ten)

Let's filter those submissions with 65 words or less:

```{r}
fesshole_wordcount %>% 
  filter(wordcount >= 65) %>% 
  ggplot(aes(wordcount)) + 
    geom_histogram(bins = 15)
```
The entries with a 100 words or more are actually below 5

How about a box plot?

```{r}
fesshole_wordcount%>% 
  ggplot(aes(wordcount)) + 
    geom_boxplot()
```

Lots of outliers after the 75 word count.  Really few long word confessions, here. Good to consider
if eliminating those confessions is needed for further analysis.

### Summary statistics for word count

Here's the summary of just the word count

```{r}
fesshole_wordcount %>% 
  select(wordcount) %>% 
  summary() 
```

The mean length is about 34 words per confession, where as 25% or less of the confessions are
23 words or less.

50% or less are 34 words long, and 75% or less are 46 words long. 

That can be appreciated in the boxplot.

Let's get the summary statistics using the `skimr`


```{r}
fesshole_wordcount %>% 
  select(wordcount) %>% 
  skimr::skim()

```

## Other measures of centrality: skewness and kurtosis

Seeing the histogram it looks that apart from the outliers, the distribution is fairly symmetrical.

The value of skewness is:

```{r}
fesshole_wordcount %>% 
  select(wordcount) %>% 
  pull() %>% 
  moments::skewness(na.rm = TRUE)
```

Which is pretty close to zero thus making it very symmetrical.

The dataset used has `r nrow(fesshole_wordcount)`.

Following the boxplot above, we can actually have the values of the outliers indicated.  This will
help us identify the rows where they happy and see what the dataset looks like without them.

```{r}
fess_out <- boxplot.stats(fesshole_wordcount$wordcount)$out
fess_out %>% sort()

```

We have `r length(fess_out)` outliers, and there are their values.  

Our set has about 41000 entries, these percentage of outlier values is practically negligible.

```{r}
length(fess_out)/nrow(fesshole_wordcount)
```

It is clsse to zero too.  

Let's remove the rows where these outlier values occur and see the distribution.

First let's identify the rows where these values occur.

```{r}
out_ind <- which(fesshole_wordcount$wordcount %in% c(fess_out))
out_ind

```

These are the rows where they occur in the dataset:

```{r}
fesshole_wordcount %>% 
  slice(out_ind) %>% 
  arrange(desc(wordcount))

```

Let's see the dataset without outliers and get the histograms and skewness.

```{r}
fesshole_wordcount %>% 
  ungroup() %>% 
  slice(-out_ind) %>% 
  ggplot(aes(wordcount)) + 
    geom_histogram(bins = 15, fill = "#0c4c8a") +
    theme_minimal()
```

And the skewness value is

```{r}
fesshole_wordcount %>% 
  slice(-out_ind) %>%
  select(wordcount) %>% 
  pull() %>% 
  moments::skewness(na.rm = TRUE)

```

The skewness falls between the -0.5, and 0.5 range so it is still fairly symmetrical.


The kurtosis value is:

```{r}
fesshole_wordcount %>% 
  slice(-out_ind) %>%
  select(wordcount) %>% 
  pull() %>% 
  moments::kurtosis(na.rm = TRUE)

```

This kurtosis value is pretty high.  Does it have to do with the fact that  I am using all of the population?

According [to this article](https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics#skewness), there is no point in gettng the values of skewness and kurtosis since they
vary a lot and are very dependent on size.  The histograms above can tell me much more information just by
looking at them.  




